## 一、模块化的模型提供商

### 1.1 重构背景与动机

这是为 Athena 设计的 LLM 请求模块，它使用 xsai，将多种请求端点通过 xsai-provider 抽象成 OpenAI 的格式，以便连接多个模型服务商。通过配置 Provider，用户可以注册多个模型，并使用模型组进行负载均衡和故障转移。主要请求逻辑在 `chat-model.ts`，是对 streamText 功能的简单包装。模型切换逻辑在 `model-switcher.ts`，这是保证模型可用性的重要环节。

在实际使用过程中我发现一些不足：
1. xsai 为保持轻量化移除了一些功能
2. 使用 OpenAI 格式抽象会导致一些特性或参数不能正常工作
3. 在一个窗口中配置所有模型不方便操作

xsai 可以看作是 ai-sdk 库的精简版，我们可以使用 ai-sdk 来替代 xsai 模块。同时，我希望将整个 model 模块独立为一个包，然后将不同的 provider 注册为插件，这样用户可以按需启用不同的 provider 插件进行单独配置，以提供更多独特的参数和功能。

### 1.2 核心架构设计

ModelService 作为一个独立的服务，向其它模块提供了与大模型交互的接口。通过配置，使用 `getChatModel/useChatGroup` 获得一个模型，构造请求负载请求大模型 API。

ModelSwitcher 是一组聊天模型（即大语言模型 LLM）的集合，拥有负载均衡等功能，使用 `getModel` 可以根据预设选项获取模型，提高模型请求的成功率，减少重试成本，这对于实时聊天的交互是很重要的。

模型切换根据配置不同，有不同的策略。负载均衡是平均调用每个模型，故障转移是优先调用第一个模型，模型不可用时尝试后续模型。当模型调用后，使用 `recordResult` 记录性能指标，作为后续选择模型的依据。除了手动选择 model 在外部实现重试逻辑，也可以直接调用 chat 接口，在内部重试。

核心包至少要包含基础接口定义，如 `BaseModel/IChatModel` 等。Provider 顾名思义，就是作为一个服务商提供可用的模型，基本上与我们的 `ProviderInstance` 一致。区别是不再统一配置，而是分散到各个插件中进行配置。ModelService 可以暴露一个注册接口，以便子插件注册模型。

### 1.3 配置与参数传递

核心插件的 ModelService 尽可能少地使用配置项，以保持配置页面简洁美观。迁移初期可以不需要 Switcher，等实现基础功能后逐步完善。

我想在 shared-model 实现抽象基类 `SharedModelProvider`，其他 provider 插件继承，简化开发流程。

核心插件负责：配置模型组，为每个模型组分配模型，在具体任务中选择配置好的模型组。

Provider 插件负责：注册并配置可用模型。

关于模型的个性化参数配置，如温度、topK 等，分为默认参数和运行时参数，默认参数使用配置值，允许被运行时参数覆盖。参数需要传递至实际请求函数 `generateText` 中，这个函数应该在 shared-model 调用，所以 provider 插件需要提供这些信息。

### 1.4 ProviderRegistry 与模型凭据

ProviderRegistry 用于注册模型，并通过 `getModel` 获取 model 配置，它返回一个对象，其中包括 `generateText` 请求参数中的 model、baseURL、apiKey 等字段，也就是 `CommonRequestOptions`，这样其他需要发送 API 请求的代码只需要解构此对象，并使用自定义参数覆盖默认参数即可：

```javascript
const model = registery.getChatModel("openai>gpt-5");
const resp = generateText({
    ...model,
    messages,
    ...
})
```

SharedProvider 是提供商插件的基类，用于转换 xsai-ext/providers 的接口，并提供一些基础功能。OpenAIProvider 是独立的插件，继承自 SharedProvider，它向 ProviderRegistry 中注册自身所拥有的模型。如果上游模型提供商支持在线获取模型，则拉取模型后添加到 ModelSchema 以供选择，如果不支持，则提供配置选项可以自己添加模型。

### 1.5 类型设计的困惑与思考

在设计 SharedProvider 时，我遇到一些困惑，我将 TModelConfig 作为泛型参数，它描述了这个提供商插件中所有模型请求时的默认值，并提供了 override 允许配置单个模型。为了区分模型支持的能力，如图片输入，我添加了 ChatModelAbility 枚举选项和 ModelInfo 接口，用于描述模型信息。

那么，ChatModelConfig 和 ChatModelInfo 有什么区别？这样设计合理吗？

我引入 ChatModelInfo 是为了在 schema 中区分模型种类，避免将图片传递给不支持图片输入的模型，同时针对不同的模型进行调整。

SharedProvider 目前是直接合并 xsai 的返回值和 `this.config.modelConfig` 中的参数，没有考虑到 override 的情况。

所以说 ModelConfig 是直接添加到 `generateText` 的输入参数，ChatModelInfo 是区分模型能力。这两者都通过 Schema 配置。

### 1.6 整体模块规划

整体架构如下：
- `packages/core/services/model` - 模型服务，用于集成 Koishi
- `packages/shared-model` - 通用抽象层，提供类型定义和工具类等
- `plugins/provider-openai` - 提供商插件，也是独立的 Koishi 插件，调用模型注册自身

这个模型服务是插件化的，每个插件都向注册表中添加一个提供商，将配置从核心插件中拆分，便于独立配置。为了方便开发插件，减少重复代码，使用 shared-model 对接口进行抽象。模型服务负责整合所有提供商插件，并按需返回模型信息以供调用。

在提供商插件中，先导入并拓展 SharedProvider，然后从 xsai（由 shared-model 再导出）导入相应的 createProvider 函数（如 createOpenAI，表示这是 OpenAI 格式的提供商）并注入 SharedProvider，这样自定义的 OpenAIProvider 就实现了 SharedProvider 接口并拥有 xsai openaiProvider 的功能。除此之外，provider-openai 还重写了配置以添加对特殊参数和行为的支持，如 OpenAI 和 Gemini 端点会使用不同的思考预算。

### 1.7 简化设计原则

核心类型定义：

```typescript
export const Modalities = ['text', 'image', 'audio', 'video', 'pdf'] as const;
export type Modality = (typeof Modalities)[number];

interface ModelCost {
  cache_read?: number;
  cache_write?: number;
  context_over_200k?: Omit<ModelCost, 'context_over_200k'>;
  input: number;
  output: number;
}

export const ProviderTypes = [
  'openai',
  'openai-compatible',
  'anthropic',
  'gemini',
  'deepseek',
] as const;

export type ProviderType = (typeof ProviderTypes)[number];
```

不再使用 ModelCapabilities、ModelMetadata、PricingInfo 以降低配置复杂度。

ProviderType 仅用于区分提供商类型，每个插件拥有独立提供商类型。不在提供商层面区分 chat/embed 等。嵌入模型单独配置，独立成插件，与文本生成模型彻底分开。现阶段优先实现文本生成模型。

不定义 IChatModel、IEmbedModel，不包装 `.chat` 请求，获取模型凭据后按需使用 xsai 的 `generateText` 等方法。简化模型服务，优化负载均衡，错误重试机制。优化模型组设计。

删除模型切换器概念，我认为当前模型切换器很不优雅。重新审视模型组设计，从配置直观性、调用方法、架构抽象层数、稳定性、灵活性等方面考虑，要为不同的任务（如上下文压缩、主代理、回复判断、记忆检索等）配置不同的模型、重试、故障转移等功能。

### 1.8 设计决策

1. 不要包装 `.chat` 方法，调用者直接使用凭据请求 `generateText`，因为模型不仅返回文本，还会返回工具调用，工具调用与 extension 模块耦合，需要显式控制对话流
2. 任务配置过于复杂，难以在 UI 界面进行配置
3. 任务执行器是过度设计，不是核心功能，现阶段是设计一个简洁的、稳定的、更加直观的模型服务，甚至模型切换器、状态追踪都可以移除

有几点关键问题：
1. 对齐模型凭据与 `generateText` 请求参数，以便直接传入请求，或者提供一个包装函数用于将模型凭据转换为请求参数
2. `generateText` 不接收代理参数，要使用代理，需要 undici 包的增强版 fetch 方法，将包装的 fetch 传入 `generateText`
3. 简化版模型配置仍然比较复杂，Koishi WebUI 配置页面会将所有支持的配置选项添加到 UI 中，每个模型都需要占用页面大部分空间。我的想法是 Koishi Schema 只定义一部分，剩余配置通过手写 JSON 实现，最后将其合并

---
